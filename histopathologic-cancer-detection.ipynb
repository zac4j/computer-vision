{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dazhengzhu/histopathologic-cancer-detection?scriptVersionId=193990516\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Histopathologic Cancer Detection","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n","metadata":{}},{"cell_type":"markdown","source":"Early and accurate cancer detection is critical for improving patient outcomes. Traditional methods relying on human expertise are time-consuming and prone to errors. The increasing volume of histopathological images further exacerbates this challenge. \n\nThis project aims to develop a deep learning-based model capable of accurately classifying histopathological images as cancerous or non-cancerous. By leveraging the Histopathologic Cancer Detection dataset, we will explore the potential of deep learning techniques to assist doctors in their diagnostic process. Successful implementation of this model could significantly enhance cancer diagnosis efficiency and contribute to advancements in medical image analysis.","metadata":{}},{"cell_type":"markdown","source":"### Setup\n\nImport Tensorflow and other necessary libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nimport PIL\nimport tensorflow as tf\n\nfrom tensorflow import keras\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nprint(tf.__version__)\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data","metadata":{}},{"cell_type":"markdown","source":"PatchCamelyon (PCam) packs the clinically-relevant task of metastasis detection into a straight-forward binary image classification task, akin to CIFAR-10 and MNIST. \n\nThe [data](https://www.kaggle.com/competitions/histopathologic-cancer-detection/data) for this project is a slightly modified version of the PCam benchmark dataset (the original PCam dataset contains duplicate images due to its probabilistic sampling, however, the version presented on Kaggle does not contain duplicates).","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"import pathlib\n\ntrain_path = '../input/histopathologic-cancer-detection/train'\ntest_path = '../input/histopathologic-cancer-detection/test'\ntrain_dir = pathlib.Path(train_path).with_suffix('')\ntest_dir = pathlib.Path(test_path).with_suffix('')\n\ntrain_imgs = list(train_dir.glob('*.tif'))\ntest_imgs = list(test_dir.glob('*.tif'))\n\nprint(len(train_imgs))\nprint(len(test_imgs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset uses total 220025 training images, and 57458 images for testing, its too large, we'll select a subset of images for training and validation later.","metadata":{}},{"cell_type":"markdown","source":"#### Preview Images\n\nNow take a look at a few pictures to get a better sense of what the dataset look like.","metadata":{}},{"cell_type":"code","source":"# Get training and testing files\ntrain_files = os.listdir(train_dir)\ntest_files = os.listdir(test_dir)\n\nprint('Training image files: ')\nprint(train_files[:10])\nprint('Testing image files: ')\nprint(test_files[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a 4x4 plot\nnrows = 4\nncols = 4\n\n# Index for iterating over images\npic_index = 0\n\n# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols * 4, nrows * 4)\n\npic_index += 8\nnext_train_pix = [os.path.join(train_dir, file)\n                for file in train_files[pic_index-8:pic_index]]\nnext_test_pix = [os.path.join(test_dir, file)\n                for file in test_files[pic_index-8:pic_index]]\n\nfor i, img_path in enumerate(next_train_pix+next_test_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing\n\nThere are total 220025 images in the train folder, it's a large dataset, we'll select a subset of them for testing and validation.\n\nThe image file type is `.tif`, which is not an supported file type with the `keras.utils.image_dataset_from_directory` function. Fortunately, the `ImageDataGenerator` could help, also we could utilize the `ImageDataGenerator` provided the `flow_from_dataframe` function to create training and validation data.","metadata":{}},{"cell_type":"markdown","source":"#### Select Train & Validation Samples\n\nTo maintain data balance, we'll randomly sample 20000 images from the dataset. Of these, 10000 will have a label of 1, while the remaining 10000 will have a label of 0.","metadata":{}},{"cell_type":"code","source":"# load labels dataframe\ndf = pd.read_csv('../input/histopathologic-cancer-detection/train_labels.csv')\n\n# Filter samples with the label\npos_df = df[df['label'] == 1]\nneg_df = df[df['label'] == 0]\n\ntrain_pos_df = pos_df.sample(n=10000, random_state=42)\ntrain_neg_df = neg_df.sample(n=10000, random_state=42)\ntrain_df = pd.concat([train_pos_df, train_neg_df])\n# Shuffle the dataframe\ntrain_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Add file type\ntrain_df.id = train_df.id + '.tif'\ntrain_df.label = train_df.label.astype(str)\n\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Setup Data Generators\n\nLet's setup training and validation data generators. The generators will yield batches of 32 images of size 96x96 and their labels.\n\nWe'll use the `keras.preprocessing.image.ImageDataGenerator` class to create generators and using the rescale parameter to normalizing the pixel values to be in the [0,1] range (from original [0, 255] range).","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define batch size and image size\nBATCH_SIZE = 32\nIMG_SIZE = (96, 96)\n\nTRAINING_SUBSET = \"training\"\nVALIDATION_SUBSET = \"validation\"\n\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nval_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\n# Create Image Generator in batches of 32\ndef create_generator(datagen: ImageDataGenerator, df: pd.DataFrame, subset: str):\n    return datagen.flow_from_dataframe(\n        directory = train_dir,\n        dataframe = df,\n        x_col = 'id',\n        y_col = 'label',\n        subset=subset,\n        seed=123,\n        target_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        class_names=['0', '1'],\n        class_mode='binary')\n\ntrain_generator = create_generator(train_datagen, train_df, TRAINING_SUBSET)\nval_generator = create_generator(val_datagen, train_df, VALIDATION_SUBSET)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"### Build a Baseline Model\n\nThe images that will go into our convnet are 96x96 color images.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras import Model\n\n# Our input feature map is 96x96x3: 96x96 stands for image height x width pixels, \n# and 3 for the three color channels: R, G, and B\nimg_input = layers.Input(shape=IMG_SIZE + (3,))\n\n# First convolution extracts 16 filters that are 3x3\n# Convolution is followed by max-pooling layer with a 2x2 window\nx = layers.Conv2D(16, 3, activation='relu')(img_input)\nx = layers.MaxPooling2D(2)(x)\n\n# Second convolution extracts 32 filters that are 3x3\n# Convolution is followed by max-pooling layer with a 2x2 window\nx = layers.Conv2D(32, 3, activation='relu')(x)\nx = layers.MaxPooling2D(2)(x)\n\n# Third convolution extracts 64 filters that are 3x3\n# Convolution is followed by max-pooling layer with a 2x2 window\nx = layers.Convolution2D(64, 3, activation='relu')(x)\nx_layers = layers.MaxPooling2D(2)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On top of it are two fully-connected layers. Because we are facing binary classification problem, we will end our network with a sigmoid activation function, so that the output of our network will be a single scalar 0 or 1, indicates the probability the image label is 0 or 1.","metadata":{}},{"cell_type":"code","source":"# Flatten feature map to a 1-dim tensor\nx = layers.Flatten()(x_layers)\n\n# Create a fully connected layer with ReLU activation and 512 hidden units\nx = layers.Dense(512, activation='relu')(x)\n\n# Create output layer with a single node and sigmoid activation\noutput = layers.Dense(1, activation='sigmoid')(x)\n\n# Create model:\n# input = input feature map\n# output = input feature map + stacked convolution/maxpooling layers + fully\n# connected layer + sigmoid output layer\nmodel = Model(img_input, output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's summarize the model:","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Output shape column shows how the size of our feature map evolves in each successive layer. We can observe that the convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the feature map.","metadata":{}},{"cell_type":"markdown","source":"Next, let's configure the sepecifications for model training. Since we're facing a binary classification problem, we'll select the `BinaryCrossentropy` loss and use the `Adam` optimizer, also monitoring the classification accuracy during the training.","metadata":{}},{"cell_type":"code","source":"def compile_model():\n    model.compile(loss=keras.losses.BinaryCrossentropy(),\n             optimizer=keras.optimizers.Adam(),\n             metrics=['accuracy'])\ncompile_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training\n\nLet's train our model for 20 epochs, using 50 batches of training data and 12 batches of validation data per epoch. The `history` object used to capture information during the training.","metadata":{}},{"cell_type":"code","source":"EPOCHS=20\ndef fit_model():\n    return model.fit(\n        train_generator,\n        epochs=EPOCHS,\n        validation_data=val_generator,\n        verbose=2\n    )\n    \nhistory = fit_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing Intermediate Representations\n\nTo get a feel for what kind of features our convnet has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the convnet.\n\nLet's pick a random training or testing image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images.","metadata":{}},{"cell_type":"code","source":"import random\n# Let's define a new Model that will take an image as input, and will output\n# intermediate representations for all layers in the previous model after\n# the first.\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\nvisualization_model = Model(img_input, successive_outputs)\n\n# Let's prepare a random input image from the training set.\nimg_path = random.choice(train_imgs)\n\nimg = keras.utils.load_img(img_path, target_size=IMG_SIZE)  # this is a PIL image\nx = keras.utils.img_to_array(img)  # Numpy array with shape (96, 96, 3)\nx = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 96, 96, 3)\n\n# Rescale by 1/255\nx /= 255\n\n# Let's run our image through our network, thus obtaining all\n# intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n\n# These are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers[1:]]\n\n# Now let's display our representations\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n  if len(feature_map.shape) == 4:\n    # Just do this for the conv / maxpool layers, not the fully-connected layers\n    n_features = feature_map.shape[-1]  # number of features in feature map\n    # The feature map has shape (1, size, size, n_features)\n    size = feature_map.shape[1]\n    # We will tile our images in this matrix\n    display_grid = np.zeros((size, size * n_features))\n    for i in range(n_features):\n      # Postprocess the feature to make it visually palatable\n      x = feature_map[0, :, :, i]\n      x -= x.mean()\n      x /= x.std()\n      x *= 64\n      x += 128\n      x = np.clip(x, 0, 255).astype('uint8')\n      # We'll tile each filter into this big horizontal grid\n      display_grid[:, i * size : (i + 1) * size] = x\n    # Display the grid\n    scale = 20. / n_features\n    plt.figure(figsize=(scale * n_features, scale))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results And Analysis","metadata":{}},{"cell_type":"markdown","source":"### Evaluate Accuracy and Loss for the Baseline Model\n\nLet's plot the training and validation accuracy and loss during training:","metadata":{}},{"cell_type":"code","source":"# Retrieve a list of accuracy results on training and validation data\n# sets for each training epoch\n\ndef preview_learning_curve():\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylabel('Accuracy')\n    plt.ylim([min(plt.ylim()),1])\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('Cross Entropy')\n    plt.ylim([0,1.0])\n    plt.title('Training and Validation Loss')\n    plt.xlabel('epoch')\n\npreview_learning_curve()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below are the observation from the figure:\n\n- Performance: Both training and validation accuracies are generally above 80%, which indicates decent performance, however, there's room for improvement.\n- Convergence: The accuracies seems to stabilize somewhat towards the end, but there's still significant fluctuation.\n- Overfitting: There's a noticeable gap between training and validation accuracy, with training accuracy often higher. This suggests some degree of overfitting.\n- Consistency: The validation accuracy shows more variability than the training accuracy, which is common but ideally should be reduced.","metadata":{}},{"cell_type":"markdown","source":"### Addressing overfitting in Model\n\nWe could employ several strategies to address overfitting:\n    \n**Data Augmentation**\n\nIn order to make the most of our few training examples, we will \"augment\" them via a number of random transformations, so that at training time, **our model will never see the exact same picture twice**.\n\nLet's create a new training data generator and apply several random transformations:","metadata":{}},{"cell_type":"code","source":"# Create training data generator with some transformations\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\ntrain_generator = create_generator(train_datagen, train_df, TRAINING_SUBSET)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dropout**\n\nAnother popular strategy is adding **dropout** layer to reduce overfitting:","metadata":{}},{"cell_type":"code","source":"# Flatten feature map to a 1-dim tensor\nx = layers.Flatten()(x_layers)\n\n# Create a fully connected layer with ReLU activation and 512 hidden units\nx = layers.Dense(512, activation='relu')(x)\n\n# Add a dropout layer rate of 0.5\nx = layers.Dropout(0.5)(x)\n\n# Create output layer with a single node and sigmoid activation\noutput = layers.Dense(1, activation='sigmoid')(x)\n\n# Create Model\nmodel = Model(img_input, output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After employ **data augmentation** and **dropout**, let's train our model and preview the results again:","metadata":{}},{"cell_type":"code","source":"# compile and train model\ncompile_model()\nhistory = fit_model()\n# preview learning curve\npreview_learning_curve()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see below improvements:\n\n- Both training and validation accuracies are close to 80%.\n- Both training and validation accuracies shows consistency than before.\n- The gap in accuracy between training and validation becomes smaller, this also indicates overfitting reduced.","metadata":{}},{"cell_type":"markdown","source":"### Submission\n\nLet's use the `model` to make prediction:","metadata":{}},{"cell_type":"code","source":"test_path = '../input/histopathologic-cancer-detection/test'\n\ndf_test = pd.read_csv('../input/histopathologic-cancer-detection/sample_submission.csv')\ndf_test['filename'] = df_test.id + '.tif'\n\ntest_datagen = ImageDataGenerator(rescale=1.0/255)\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe = df_test,\n    directory = test_path,\n    x_col = 'filename',\n    batch_size = BATCH_SIZE,\n    shuffle = False,\n    class_mode = None,\n    target_size = IMG_SIZE,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then let's use CNN model to make prediction: ","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(test_generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_pred = np.where(predictions > 0.5, 1, 0)\nlabel_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply predictions to submission dataframe:","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('../input/histopathologic-cancer-detection/sample_submission.csv')\nsubmission.label = label_pred\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write dataframe to submission csv file:","metadata":{}},{"cell_type":"code","source":"submission.to_csv('submission.csv', header = True, index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"This project aimed to explore the CNN model for Histopathologic Cancer Detection. The model achieved 80% average accuracy on the validation dataset, indicates decent performance.\n\nDuring training the CNN model, we experienced overfitting reduce overfitting techniques such as data augmentation and add dropout layer, these techniques show good results in reducing overfitting. However, the accuracy is not very high. In the future, we can try some SOTA(state-of-the-art) visual model to further improve the accuracy.\n\nAlthough the model shows good accuracy and consistency, there is still fluctuation in accuracy. There may be some noise in the dataset. We can further adopt anomaly detection methods to clean up the anomalies and reduce the fluctuation in accuracy.","metadata":{}},{"cell_type":"markdown","source":"## Reference","metadata":{}},{"cell_type":"markdown","source":"- [Keras Functional API](https://www.tensorflow.org/guide/keras/functional)\n- [Tensorflow: Classification](https://www.tensorflow.org/tutorials/images/classification)\n- [Tensorflow: Data Augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation)\n- [Tensorflow: Transfer learning and fine-tuning](https://www.tensorflow.org/tutorials/images/transfer_learning)","metadata":{}}]}